lora_r = 16
lora_alpha = 64 
lora_dropout = 0.2 
bias = "lora_only"
task_type = "CAUSAL_LM"
per_device_train_batch_size = 1 
gradient_accumulation_steps = 6 
learning_rate = 6e-4 
max_steps = 250 
warmup_steps = 40 
optim = "paged_adamw_8bit"
