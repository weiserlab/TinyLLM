{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the dataset\n",
    "- This dataset is hosted on [GDrive](http://bit.ly/2VGEeWN)\n",
    "- Navigate to the data folder and download the folder\n",
    "- Extract the downloaded zip file and move the 40 folders in it to ```Swim``` folder\n",
    "- run the notebook and find the processed data in the ```data``` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Function to process each file\n",
    "def process_file(file_path, n):\n",
    "    data = pd.read_csv(file_path)\n",
    "    \n",
    "    # Extract relevant columns\n",
    "    acc_columns = ['ACC_0', 'ACC_1', 'ACC_2']\n",
    "    label_column = 'label'\n",
    "    \n",
    "    # Map label numbers to actual style names\n",
    "    label_map = {0: 'transition', 1: 'freestyle', 2: 'breaststroke', 3: 'backstroke', 4: 'butterfly', 5: 'transition'}\n",
    "    data['label'] = data['label'].map(label_map)\n",
    "    \n",
    "    # List to store processed rows\n",
    "    rows = []\n",
    "    \n",
    "    # Group data by continuous labels\n",
    "    current_label = None\n",
    "    current_group = []\n",
    "    \n",
    "    for i, row in data.iterrows():\n",
    "        label = row['label']\n",
    "        \n",
    "        if label != current_label:\n",
    "            if current_group:\n",
    "                rows.extend(process_group(current_group, n, current_label, acc_columns))\n",
    "            current_group = []\n",
    "            current_label = label\n",
    "            \n",
    "        current_group.append(row)\n",
    "    \n",
    "    # Process the last group\n",
    "    if current_group:\n",
    "        rows.extend(process_group(current_group, n, current_label, acc_columns))\n",
    "    \n",
    "    return rows\n",
    "\n",
    "# Function to process a group of continuous label data\n",
    "def process_group(group, n, label, acc_columns):\n",
    "    rows = []\n",
    "    num_entries = len(group)\n",
    "    \n",
    "    # Convert the group (list of rows) into a DataFrame\n",
    "    group_df = pd.DataFrame(group)\n",
    "    \n",
    "    # Split into chunks of size n\n",
    "    for i in range(0, num_entries, n):\n",
    "        chunk = group_df.iloc[i:i+n]\n",
    "        \n",
    "        if len(chunk) < n:\n",
    "            # Handle the case where the last chunk is smaller than n\n",
    "            previous_chunk = group_df.iloc[i-n:i] if i >= n else group_df.iloc[:i]\n",
    "            chunk = pd.concat([previous_chunk, chunk]).tail(n)  # Get the last n readings\n",
    "        \n",
    "        acc_values = chunk[acc_columns].round(0).astype(int)\n",
    "        input_str = f\"X: {list(acc_values['ACC_0'])}\\nY: {list(acc_values['ACC_1'])}\\nZ: {list(acc_values['ACC_2'])}\"\n",
    "        rows.append([input_str, label])\n",
    "    \n",
    "    return rows\n",
    "\n",
    "# Directory paths and parameters\n",
    "data_dir = \"Swim\"\n",
    "n = 100\n",
    "\n",
    "# List to hold all data\n",
    "all_data = []\n",
    "\n",
    "# Process each swimmer folder\n",
    "for swimmer in os.listdir(data_dir):\n",
    "    swimmer_path = os.path.join(data_dir, swimmer)\n",
    "    if os.path.isdir(swimmer_path):\n",
    "        for file in os.listdir(swimmer_path):\n",
    "            file_path = os.path.join(swimmer_path, file)\n",
    "            if file.endswith(\".csv\"):\n",
    "                all_data.extend(process_file(file_path, n))\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(all_data, columns=['Input', 'Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preprocessing completed!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.DataFrame(all_data, columns=['input', 'output'])\n",
    "df = df.dropna()  # Drop rows with NaN values\n",
    "\n",
    "\n",
    "# Split into Train, Test, Val\n",
    "train, test = train_test_split(df, test_size=0.3, stratify=df['Label'])\n",
    "val, test = train_test_split(test, test_size=2/3, stratify=test['Label'])\n",
    "\n",
    "# Save to CSV files\n",
    "train.to_csv('data/train.csv', index=False)\n",
    "val.to_csv('data/validation.csv', index=False)\n",
    "test.to_csv('data/test.csv', index=False)\n",
    "\n",
    "print(\"Data preprocessing completed!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
