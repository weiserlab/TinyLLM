{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:46: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:75: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:46: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:75: SyntaxWarning: invalid escape sequence '\\s'\n",
      "/tmp/ipykernel_54331/540691241.py:46: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  label_data = pd.read_csv(label_file_path, sep=\"\\s+\", header=None)\n",
      "/tmp/ipykernel_54331/540691241.py:75: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  for i, chunk in enumerate(pd.read_csv(file_path, sep=\"\\s+\", header=None, chunksize=chunk_size)):\n",
      "Processing 070717: 100%|██████████| 4/4 [04:02<00:00, 60.73s/it]\n",
      "Processing all dates:  12%|█▎        | 1/8 [04:07<28:52, 247.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Motion data for 070717 exported to 070717_motion_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 140617: 100%|██████████| 4/4 [03:22<00:00, 50.73s/it]\n",
      "Processing all dates:  25%|██▌       | 2/8 [07:33<22:19, 223.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Motion data for 140617 exported to 140617_motion_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 260617: 100%|██████████| 4/4 [03:36<00:00, 54.25s/it]\n",
      "Processing all dates:  38%|███▊      | 3/8 [11:13<18:28, 221.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Motion data for 260617 exported to 260617_motion_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 180717: 100%|██████████| 4/4 [02:58<00:00, 44.56s/it]\n",
      "Processing all dates:  50%|█████     | 4/8 [14:15<13:43, 205.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Motion data for 180717 exported to 180717_motion_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 220617: 100%|██████████| 4/4 [04:03<00:00, 60.98s/it]\n",
      "Processing all dates:  62%|██████▎   | 5/8 [18:22<11:01, 220.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Motion data for 220617 exported to 220617_motion_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 030717: 100%|██████████| 4/4 [02:55<00:00, 43.81s/it]\n",
      "Processing all dates:  75%|███████▌  | 6/8 [21:20<06:52, 206.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Motion data for 030717 exported to 030717_motion_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 140717: 100%|██████████| 4/4 [03:13<00:00, 48.33s/it]\n",
      "Processing all dates:  88%|████████▊ | 7/8 [24:36<03:22, 202.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Motion data for 140717 exported to 140717_motion_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 270617: 100%|██████████| 4/4 [03:11<00:00, 47.86s/it]\n",
      "Processing all dates: 100%|██████████| 8/8 [27:51<00:00, 208.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Motion data for 270617 exported to 270617_motion_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define label mappings for fine labels according to the document provided, using '' for ignored labels\n",
    "FINE_LABEL_MAP = {\n",
    "    0: '',\n",
    "    1: 'Still;Stand;Outside',\n",
    "    2: 'Still;Stand;Inside',\n",
    "    3: 'Still;Sit;Outside',\n",
    "    4: 'Still;Sit;Inside',\n",
    "    5: 'Walking;Outside',\n",
    "    6: 'Walking;Inside',\n",
    "    7: 'Run',\n",
    "    8: 'Bike',\n",
    "    9: 'Car;Driver',\n",
    "    10: 'Car;Passenger',\n",
    "    11: 'Bus;Stand',\n",
    "    12: 'Bus;Sit',\n",
    "    13: 'Bus;Up;Stand',\n",
    "    14: 'Bus;Up;Sit',\n",
    "    15: 'Train;Stand',\n",
    "    16: 'Train;Sit',\n",
    "    17: 'Subway;Stand',\n",
    "    18: 'Subway;Sit'\n",
    "}\n",
    "\n",
    "# Number of threads for parallel processing, adjustable variable\n",
    "NUM_THREADS = 1  # Change this value as needed\n",
    "\n",
    "def read_fine_labels(label_file_path):\n",
    "    \"\"\"\n",
    "    Reads the Label.txt file and extracts fine labels from the third column, converting them to descriptive text.\n",
    "    Filters out empty labels.\n",
    "    \n",
    "    Parameters:\n",
    "        label_file_path (str): The path to the Label.txt file.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of fine descriptive labels corresponding to each time step.\n",
    "    \"\"\"\n",
    "    fine_labels = []\n",
    "    try:\n",
    "        # Read the labels from the Label.txt file\n",
    "        label_data = pd.read_csv(label_file_path, sep=\"\\s+\", header=None)\n",
    "        \n",
    "        # Convert numeric labels to descriptive text using the provided map\n",
    "        fine_labels = label_data.iloc[:, 2].apply(lambda x: FINE_LABEL_MAP.get(x, '')).tolist()\n",
    "\n",
    "        # Keep only non-empty labels\n",
    "        fine_labels = [label if label else None for label in fine_labels]\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {label_file_path}: {e}\")\n",
    "\n",
    "    return fine_labels\n",
    "\n",
    "def process_motion_data_chunked(file_path, fine_labels, chunk_size=100000):\n",
    "    \"\"\"\n",
    "    Processes motion data in chunks to avoid memory overload and writes to a CSV file.\n",
    "    \n",
    "    Parameters:\n",
    "        file_path (str): The path to the motion data file.\n",
    "        fine_labels (list): The list of fine descriptive labels corresponding to each time step.\n",
    "        chunk_size (int): The number of rows to process in each chunk.\n",
    "    \n",
    "    Returns:\n",
    "        list: List of tuples containing sensor data and the corresponding fine label.\n",
    "    \"\"\"\n",
    "    # Create an empty list to hold chunk results\n",
    "    chunk_results = []\n",
    "\n",
    "    try:\n",
    "        # Read the file in chunks\n",
    "        for i, chunk in enumerate(pd.read_csv(file_path, sep=\"\\s+\", header=None, chunksize=chunk_size)):\n",
    "            # Convert each row of sensor data to a list including time\n",
    "            sensor_data = chunk.apply(lambda row: [row[0]] + row[1:].tolist(), axis=1)  # Include time\n",
    "            \n",
    "            # Synchronize fine labels with sensor data for the current chunk\n",
    "            chunk_fine_labels = fine_labels[i * chunk_size: i * chunk_size + len(chunk)]\n",
    "            \n",
    "            # Collect the chunk's results, skipping rows where the label is empty\n",
    "            for data, label in zip(sensor_data, chunk_fine_labels):\n",
    "                if label is not None:  # Skip rows where the label is empty\n",
    "                    chunk_results.append((data, label))\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "\n",
    "    return chunk_results\n",
    "\n",
    "def process_motion_data_for_date(record_path):\n",
    "    \"\"\"\n",
    "    Processes motion data files for a specific recording session (date) and exports them to a CSV file.\n",
    "    \n",
    "    Parameters:\n",
    "        record_path (str): The directory path containing the .txt files for a specific date.\n",
    "    \"\"\"\n",
    "    # Output file path\n",
    "    record_folder = os.path.basename(record_path)\n",
    "    output_csv = f\"{record_folder}_motion_data.csv\"\n",
    "    \n",
    "    # Read the fine labels from the corresponding Label.txt file\n",
    "    label_file_path = os.path.join(record_path, 'Label.txt')\n",
    "    fine_labels = read_fine_labels(label_file_path)\n",
    "\n",
    "    # Write the header to the CSV file first\n",
    "    with open(output_csv, 'w') as f:\n",
    "        f.write('Sensor Data,Label\\n')\n",
    "\n",
    "    # Get the list of _Motion.txt files\n",
    "    motion_files = [file for file in os.listdir(record_path) if \"_Motion.txt\" in file]\n",
    "\n",
    "    # Process each motion data file in parallel\n",
    "    with ThreadPoolExecutor(max_workers=NUM_THREADS) as executor:\n",
    "        futures = [executor.submit(process_motion_data_chunked, os.path.join(record_path, file), fine_labels) for file in motion_files]\n",
    "\n",
    "        for future in tqdm(futures, desc=f\"Processing {record_folder}\"):\n",
    "            chunk_results = future.result()\n",
    "\n",
    "            # Write chunk results to CSV\n",
    "            with open(output_csv, 'a') as f:\n",
    "                for sensor_data, label in chunk_results:\n",
    "                    # Convert list to space-separated string for sensor data\n",
    "                    sensor_data_str = ' '.join(map(str, sensor_data))\n",
    "                    f.write(f'\"[{sensor_data_str}]\",{label}\\n')\n",
    "\n",
    "    print(f\"Motion data for {record_folder} exported to {output_csv}\")\n",
    "\n",
    "def process_motion_data_per_date(root_dir):\n",
    "    \"\"\"\n",
    "    Processes motion data files for User1 one by one and exports each date's data to a separate CSV file.\n",
    "    \n",
    "    Parameters:\n",
    "        root_dir (str): The root directory containing the User1 subdirectories.\n",
    "    \"\"\"\n",
    "    # Get all recording session paths (subdirectories) for User1\n",
    "    record_paths = [os.path.join(root_dir, folder) for folder in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, folder))]\n",
    "\n",
    "    # Process each record path serially\n",
    "    for record_path in tqdm(record_paths, desc=\"Processing all dates\"):\n",
    "        process_motion_data_for_date(record_path)\n",
    "\n",
    "# Define the root directory containing the User1 dataset\n",
    "root_dir = 'User1/'  # Adjust this to your actual dataset path\n",
    "\n",
    "# Call the function to process motion data and export each date's data to separate CSV files\n",
    "process_motion_data_per_date(root_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
